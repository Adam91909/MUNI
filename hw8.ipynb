{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyJNsvSFac3K"
   },
   "source": [
    "# Train CNN Classifier on human_ocr_ensembl dataset\n",
    "\n",
    "The dataset comes from the [Genomic Benchmarks](https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks). Best reaults achieved are reported in these [tables](https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks/tree/main/experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: genomic-benchmarks in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.9)\n",
      "Requirement already satisfied: optuna in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: Dataset in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.6.2)\n",
      "Requirement already satisfied: biopython>=1.79 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from genomic-benchmarks) (1.84)\n",
      "Requirement already satisfied: requests>=2.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from genomic-benchmarks) (2.32.3)\n",
      "Requirement already satisfied: pip>=20.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from genomic-benchmarks) (24.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from genomic-benchmarks) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from genomic-benchmarks) (2.1.4)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from genomic-benchmarks) (4.66.5)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from genomic-benchmarks) (6.0.2)\n",
      "Requirement already satisfied: gdown>=4.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from genomic-benchmarks) (5.2.0)\n",
      "Requirement already satisfied: yarl in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from genomic-benchmarks) (1.9.11)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (1.4.54)\n",
      "Requirement already satisfied: banal>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from Dataset) (1.0.6)\n",
      "Requirement already satisfied: Mako in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions>=4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gdown>=4.2.0->genomic-benchmarks) (4.12.3)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gdown>=4.2.0->genomic-benchmarks) (3.15.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.1.4->genomic-benchmarks) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.1.4->genomic-benchmarks) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.1.4->genomic-benchmarks) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->genomic-benchmarks) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->genomic-benchmarks) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->genomic-benchmarks) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.23.0->genomic-benchmarks) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: multidict>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yarl->genomic-benchmarks) (6.0.5)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.4->genomic-benchmarks) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.2.0->genomic-benchmarks) (2.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests[socks]->gdown>=4.2.0->genomic-benchmarks) (1.7.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install genomic-benchmarks optuna Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from genomic_benchmarks.data_check import info\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset `human_enhancers_cohn` has 2 classes: negative, positive.\n",
      "\n",
      "All lengths of genomic intervals equals 500.\n",
      "\n",
      "Totally 27791 sequences have been found, 20843 for training and 6948 for testing.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>10422</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>10421</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          train  test\n",
       "negative  10422  3474\n",
       "positive  10421  3474"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info(\"human_enhancers_cohn\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"katarinagresova/Genomic_Benchmarks_human_enhancers_cohn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq': 'TGGTGGTACTTGTCAGGACTTGGAGCAGCAGGTGCAAGATTTAGTGGGTTGGTTTTAGAATATCTGCTTGGAAAGTGGAAAAACTCAATGGATCATCTAGACTTTGGAATTTATCTCCTTCCCCACTTCTCCACTCCCCCAACAACAACAACAACAACAATGACAACAAAAACACCTGGAATAAACAGGTCATACAACGAGGTAGTTGATAGAATAATGTACTTTCCTTTCAGGCACCCCTTGGAGGAGGCAGATTCTGCCCTTTAAGCTGAATCTGCCTTTCCTGCATTTCCTGAAACTCCTGCATTTCCTGAAATCTTCCTGTATTTTCCTGAAATTTCCTGCCATTCCTGAAACTTTAAGGTAACTGTGTCATTAAAGGAAGGAGAGAAGGGAAGTATTAGGACTGCAGATTTGGGGTGCATGATCAGCCTGGCTCTGAGCTTGCAGACTCCCAGAGTCAGGGAAGGGAGGAGCCACCAGCAACCTTGTGGCTTACT',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, max_length=500):\n",
    "    one_hot = torch.zeros((4, max_length), dtype=torch.float32)\n",
    "    \n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    \n",
    "    for i, nucleotide in enumerate(sequence[:max_length]):\n",
    "        if nucleotide in mapping:\n",
    "            one_hot[mapping[nucleotide], i] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "    \n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.dataset[idx]['seq']\n",
    "        label = self.dataset[idx]['label']\n",
    "        encoded_seq = one_hot_encode(seq)\n",
    "        return encoded_seq, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset[\"train\"].with_format(\"torch\")\n",
    "ds = DNADataset(ds)\n",
    "\n",
    "train_size = int(0.8 * len(ds))\n",
    "val_size = len(ds) - train_size\n",
    "\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNAClassifierCNN(nn.Module):\n",
    "    def __init__(self, kernel_size=5, dropout_rate=0.3):\n",
    "        super(DNAClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=kernel_size, stride=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=kernel_size, stride=1)\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.count_flatten_size(), 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def count_flatten_size(self):\n",
    "        dummy_input = torch.zeros(1, 4, 500)\n",
    "        dummy_output = self.pool(self.conv2(self.pool(self.conv1(dummy_input))))\n",
    "        return dummy_output.view(-1).size(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.reshape(x.size(0), -1) \n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.float().to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs.to(DEVICE))\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            labels = labels.float().to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = (outputs.squeeze() > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(model, epochs, lr, optimizer, train_loader, val_loader, criterion):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            labels = labels.float().to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()  \n",
    "            outputs = model(inputs.to(DEVICE))  \n",
    "            loss = criterion(outputs.squeeze(), labels)  \n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "            \n",
    "            train_loss += loss.item()  \n",
    "        \n",
    "        model.eval()  \n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, labels = batch\n",
    "                labels = labels.float().to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs.to(DEVICE))  \n",
    "                loss = criterion(outputs.squeeze(), labels) \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = (outputs.squeeze() > 0.5).float() \n",
    "                correct += (preds == labels).sum().item()  \n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = correct / len(val_loader.dataset)\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} - \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "            f\"Val Accuracy: {val_accuracy:.2%}\"\n",
    "        )\n",
    "    \n",
    "    return avg_val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.6135, Val Loss: 0.5910, Val Accuracy: 67.79%\n",
      "Epoch 2/5 - Train Loss: 0.5927, Val Loss: 0.5891, Val Accuracy: 67.98%\n",
      "Epoch 3/5 - Train Loss: 0.5790, Val Loss: 0.5847, Val Accuracy: 68.89%\n",
      "Epoch 4/5 - Train Loss: 0.5595, Val Loss: 0.5786, Val Accuracy: 69.63%\n",
      "Epoch 5/5 - Train Loss: 0.5354, Val Loss: 0.6074, Val Accuracy: 66.39%\n",
      "Final Validation Loss: 0.6074, Accuracy: 66.39%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "ds = dataset[\"train\"].with_format(\"torch\")\n",
    "ds = DNADataset(ds)\n",
    "\n",
    "train_size = int(0.8 * len(ds))\n",
    "val_size = len(ds) - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(ds, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "model = DNAClassifierCNN().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "criterion = torch.nn.BCELoss() \n",
    "\n",
    "avg_loss, accuracy = evaluation_loop(\n",
    "    model=model,\n",
    "    epochs=5,\n",
    "    lr=0.001,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion\n",
    ")\n",
    "\n",
    "print(f\"Final Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam optimization\n",
    "\n",
    "Let's try to optimize the learning rate, number of training epochs and size of the convolution kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_float('learning_rate', 0.00001, 0.01)\n",
    "    epochs = trial.suggest_int('epochs', 5, 10)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 3, 5)\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2)\n",
    "    optimizer_type = trial.suggest_categorical('optimizer', ['AdamW', 'RMSprop'])\n",
    "\n",
    "    print(f\"LR: {lr}, Epochs: {epochs}, Kernel size: {kernel_size}, Batch size: {batch_size}, Dropout rate: {dropout_rate}, Weight decay: {weight_decay}, Optimizer: {optimizer_type}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DNAClassifierCNN(kernel_size=kernel_size, dropout_rate=dropout_rate).to(DEVICE)\n",
    "\n",
    "    # Select optimizer based on trial\n",
    "    if optimizer_type == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:  # RMSprop\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Evaluate the model using evaluation_loop\n",
    "    _, acc = evaluation_loop(model, epochs, lr, optimizer, train_loader, val_loader, nn.BCELoss())\n",
    "    return acc  # Return the accuracy to be optimized by Optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 13:33:17,200] A new study created in memory with name: no-name-316221a0-210e-4c56-8baf-fc180dd25f4f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.005892030384720968, Epochs: 9, Kernel size: 4, Batch size: 55, Dropout rate: 0.25733416864910474, Weight decay: 0.0063130231635391155, Optimizer: RMSprop\n",
      "Epoch 1/9 - Train Loss: 0.7626, Val Loss: 0.6067, Val Accuracy: 66.95%\n",
      "Epoch 2/9 - Train Loss: 0.6152, Val Loss: 0.6811, Val Accuracy: 57.83%\n",
      "Epoch 3/9 - Train Loss: 0.6127, Val Loss: 0.5988, Val Accuracy: 67.26%\n",
      "Epoch 4/9 - Train Loss: 0.6077, Val Loss: 0.5939, Val Accuracy: 67.14%\n",
      "Epoch 5/9 - Train Loss: 0.6032, Val Loss: 0.5892, Val Accuracy: 68.31%\n",
      "Epoch 6/9 - Train Loss: 0.5965, Val Loss: 0.5802, Val Accuracy: 69.23%\n",
      "Epoch 7/9 - Train Loss: 0.5970, Val Loss: 0.6615, Val Accuracy: 62.94%\n",
      "Epoch 8/9 - Train Loss: 0.5960, Val Loss: 0.6534, Val Accuracy: 62.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 13:44:26,425] Trial 0 finished with value: 0.5325017989925641 and parameters: {'learning_rate': 0.005892030384720968, 'epochs': 9, 'kernel_size': 4, 'batch_size': 55, 'dropout_rate': 0.25733416864910474, 'weight_decay': 0.0063130231635391155, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.5325017989925641.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/9 - Train Loss: 0.5937, Val Loss: 0.8622, Val Accuracy: 53.25%\n",
      "LR: 0.008400862231711404, Epochs: 8, Kernel size: 3, Batch size: 89, Dropout rate: 0.10933790352794137, Weight decay: 0.0019893386317083425, Optimizer: RMSprop\n",
      "Epoch 1/8 - Train Loss: 1.2581, Val Loss: 0.6185, Val Accuracy: 65.68%\n",
      "Epoch 2/8 - Train Loss: 0.6159, Val Loss: 0.6831, Val Accuracy: 60.28%\n",
      "Epoch 3/8 - Train Loss: 0.6107, Val Loss: 0.6084, Val Accuracy: 66.06%\n",
      "Epoch 4/8 - Train Loss: 0.6089, Val Loss: 0.5916, Val Accuracy: 68.12%\n",
      "Epoch 5/8 - Train Loss: 0.6062, Val Loss: 0.6299, Val Accuracy: 63.73%\n",
      "Epoch 6/8 - Train Loss: 0.6110, Val Loss: 0.6811, Val Accuracy: 58.93%\n",
      "Epoch 7/8 - Train Loss: 0.6030, Val Loss: 0.6186, Val Accuracy: 66.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 13:54:21,724] Trial 1 finished with value: 0.6665867114415928 and parameters: {'learning_rate': 0.008400862231711404, 'epochs': 8, 'kernel_size': 3, 'batch_size': 89, 'dropout_rate': 0.10933790352794137, 'weight_decay': 0.0019893386317083425, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.6665867114415928.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8 - Train Loss: 0.6094, Val Loss: 0.6134, Val Accuracy: 66.66%\n",
      "LR: 0.006141293521589701, Epochs: 6, Kernel size: 3, Batch size: 37, Dropout rate: 0.4858476580077141, Weight decay: 0.008534651415073886, Optimizer: AdamW\n",
      "Epoch 1/6 - Train Loss: 0.6215, Val Loss: 0.6229, Val Accuracy: 66.15%\n",
      "Epoch 2/6 - Train Loss: 0.6002, Val Loss: 0.6261, Val Accuracy: 65.56%\n",
      "Epoch 3/6 - Train Loss: 0.5964, Val Loss: 0.5934, Val Accuracy: 68.31%\n",
      "Epoch 4/6 - Train Loss: 0.5895, Val Loss: 0.5825, Val Accuracy: 68.27%\n",
      "Epoch 5/6 - Train Loss: 0.5872, Val Loss: 0.5836, Val Accuracy: 70.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 14:01:28,610] Trial 2 finished with value: 0.6946509954425522 and parameters: {'learning_rate': 0.006141293521589701, 'epochs': 6, 'kernel_size': 3, 'batch_size': 37, 'dropout_rate': 0.4858476580077141, 'weight_decay': 0.008534651415073886, 'optimizer': 'AdamW'}. Best is trial 2 with value: 0.6946509954425522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/6 - Train Loss: 0.5827, Val Loss: 0.5823, Val Accuracy: 69.47%\n",
      "LR: 0.0007016479903258781, Epochs: 10, Kernel size: 5, Batch size: 112, Dropout rate: 0.2115460523297264, Weight decay: 0.006574428090540692, Optimizer: RMSprop\n",
      "Epoch 1/10 - Train Loss: 0.6195, Val Loss: 0.6039, Val Accuracy: 66.51%\n",
      "Epoch 2/10 - Train Loss: 0.6043, Val Loss: 0.6030, Val Accuracy: 67.02%\n",
      "Epoch 3/10 - Train Loss: 0.6015, Val Loss: 0.6078, Val Accuracy: 66.59%\n",
      "Epoch 4/10 - Train Loss: 0.6010, Val Loss: 0.6039, Val Accuracy: 66.68%\n",
      "Epoch 5/10 - Train Loss: 0.5990, Val Loss: 0.6005, Val Accuracy: 67.09%\n",
      "Epoch 6/10 - Train Loss: 0.5980, Val Loss: 0.6230, Val Accuracy: 64.64%\n",
      "Epoch 7/10 - Train Loss: 0.5963, Val Loss: 0.6828, Val Accuracy: 58.60%\n",
      "Epoch 8/10 - Train Loss: 0.5957, Val Loss: 0.6311, Val Accuracy: 63.83%\n",
      "Epoch 9/10 - Train Loss: 0.5947, Val Loss: 0.6024, Val Accuracy: 67.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-19 14:14:54,755] Trial 3 finished with value: 0.5919884864475894 and parameters: {'learning_rate': 0.0007016479903258781, 'epochs': 10, 'kernel_size': 5, 'batch_size': 112, 'dropout_rate': 0.2115460523297264, 'weight_decay': 0.006574428090540692, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.6946509954425522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Train Loss: 0.5927, Val Loss: 0.7199, Val Accuracy: 59.20%\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.006141293521589701, 'epochs': 6, 'kernel_size': 3, 'batch_size': 37, 'dropout_rate': 0.4858476580077141, 'weight_decay': 0.008534651415073886, 'optimizer': 'AdamW'}\n",
      "Best value (validation AU PRC): 0.6946509954425522\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "print(f\"Best value (validation AU PRC): {study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'val' split found. Creating one from the 'train' split.\n",
      "Epoch 1/6 - Train Loss: 0.6278, Val Loss: 0.6046, Val Accuracy: 66.80%\n",
      "Epoch 2/6 - Train Loss: 0.6004, Val Loss: 0.5936, Val Accuracy: 68.10%\n",
      "Epoch 3/6 - Train Loss: 0.5887, Val Loss: 0.5797, Val Accuracy: 68.89%\n",
      "Epoch 4/6 - Train Loss: 0.5813, Val Loss: 0.5761, Val Accuracy: 69.08%\n",
      "Epoch 5/6 - Train Loss: 0.5807, Val Loss: 0.5966, Val Accuracy: 67.98%\n",
      "Epoch 6/6 - Train Loss: 0.5813, Val Loss: 0.5973, Val Accuracy: 67.19%\n",
      "Validation Loss: 0.5973212175241863, Validation Accuracy: 0.671863756296474\n",
      "Test Loss: 0.6014408715274355, Test Accuracy: 0.6672423719055843\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "if not hasattr(study, 'best_params'):\n",
    "    raise ValueError(\"study object or best_params not found\")\n",
    "\n",
    "best_model = DNAClassifierCNN(\n",
    "    kernel_size=study.best_params['kernel_size'],\n",
    "    dropout_rate=study.best_params['dropout_rate']\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    best_model.parameters(),\n",
    "    lr=study.best_params['learning_rate'],\n",
    "    weight_decay=study.best_params['weight_decay']\n",
    ")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "if 'train' not in dataset or 'test' not in dataset:\n",
    "    raise ValueError(\"Dataset must have 'train' and 'test' splits\")\n",
    "\n",
    "if 'val' not in dataset:\n",
    "    print(\"No 'val' split found. Creating one from the 'train' split.\")\n",
    "    train_data = dataset['train']\n",
    "    train_size = int(0.8 * len(train_data))  # 80% for training\n",
    "    val_size = len(train_data) - train_size  # 20% for validation\n",
    "    train_ds, val_ds = torch.utils.data.random_split(train_data, [train_size, val_size])\n",
    "else:\n",
    "    train_ds = dataset['train']\n",
    "    val_ds = dataset['val']\n",
    "\n",
    "train_loader = DataLoader(DNADataset(train_ds), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(DNADataset(val_ds), batch_size=32, shuffle=False)\n",
    "\n",
    "avg_loss, accuracy = evaluation_loop(\n",
    "    best_model,\n",
    "    epochs=study.best_params['epochs'],\n",
    "    lr=study.best_params['learning_rate'],\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion\n",
    ")\n",
    "\n",
    "print(f\"Validation Loss: {avg_loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "test_loader = DataLoader(DNADataset(dataset[\"test\"]), batch_size=32, shuffle=False)\n",
    "\n",
    "test_loss, test_accuracy = test_model(best_model, test_loader, criterion)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "interpreter": {
   "hash": "af167c304fdc99884e31a029277e630a5b00036f91292350fffdeed1d15b16ff"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
