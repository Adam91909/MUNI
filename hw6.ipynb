{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3397422-28b2-43bf-a057-37a41c0379aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a435a-50b3-4b49-8d21-52155538e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624a305a-33cf-428a-b3e5-28a4a4f1bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136191a7-c9ad-4c42-8f87-a7f40374f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own dataset\n",
    "dataset = load_dataset(\"Diplomkaazvposlednimsemestru/MUNIAI\")\n",
    "\n",
    "# Make validation split\n",
    "dataset = dataset['train'].train_test_split(test_size=0.0015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9326dae-f49c-4570-ae4c-016fbcb74d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gpt-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a23106-194b-450e-9564-f7a0831837cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8ba5d545104202b528d86bd537f3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798c292233df4bc3801f238a7a38ef67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['author', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3349\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['author', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(text=example[\"text\"])\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns='text')\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5235011a-ab48-49dc-947a-a6025dc05f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1172\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def concatenate_and_chunk(dataset, chunk_size=512):\n",
    "    # Flatten all `input_ids` into a single list\n",
    "    all_input_ids = list(chain(*dataset[\"input_ids\"]))\n",
    "    \n",
    "    # Create chunks of `chunk_size`\n",
    "    chunks = [all_input_ids[i:i + chunk_size] for i in range(0, len(all_input_ids), chunk_size)]\n",
    "    \n",
    "    # Only keep chunks that are exactly of length `chunk_size`\n",
    "    chunks = [chunk for chunk in chunks if len(chunk) == chunk_size]\n",
    "    \n",
    "    # Create a new dataset with only the `input_ids` chunks\n",
    "    return Dataset.from_dict({\"input_ids\": chunks})\n",
    "\n",
    "# Apply this function to each split (train and test) in the DatasetDict\n",
    "chunked_ds = DatasetDict({\n",
    "    split: concatenate_and_chunk(split_ds, chunk_size=512)\n",
    "    for split, split_ds in tokenized_ds.items()\n",
    "})\n",
    "\n",
    "chunked_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7a1a83-b819-49c1-86a3-3acb34279643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator joins chunks into batches\n",
    "# see https://huggingface.co/docs/transformers/en/main_classes/data_collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "324832fa-20d7-41f2-8d48-d75d999ae61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPRAVIL JSEM KVULI CPU\n",
    "\n",
    "# Define the model configuration for the smallest GPT-2\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),      # Standard GPT-2 vocab size 50257\n",
    "    n_positions=512,                # Context size (512 is enough for small-scale models)\n",
    "    n_embd=768,                     # Embedding size\n",
    "    n_layer=12,                     # Number of transformer layers\n",
    "    n_head=12,                      # Number of attention heads\n",
    ")\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df5fba80-6d73-49c1-b8ab-733920c81e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Define the perplexity metric\n",
    "def compute_metrics(eval_pred):\n",
    "    # `eval_pred` is a tuple of (logits, labels)\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Convert logits and labels to PyTorch tensors if they are NumPy arrays\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.tensor(logits)\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "    # Shift labels so that tokens align for calculating loss\n",
    "    shift_labels = labels[:, 1:].reshape(-1)\n",
    "    shift_logits = logits[:, :-1, :].reshape(-1, logits.shape[-1])\n",
    "\n",
    "    # Calculate the cross-entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding tokens\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return {\"perplexity\": perplexity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "375bc4b0-164c-45f7-9448-c186fa378f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPRAVIL JSEM KVULI CPU\n",
    "\n",
    "# Set this according to size of your dataset\n",
    "# You should train for at least 15 mins on A10 GPU to get something reasonable\n",
    "TRAIN_EPOCHS = 15\n",
    "\n",
    "SAVE_STEPS = 300\n",
    "EVAL_STEPS = SAVE_STEPS // 2\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-training\",  # Directory to save the model checkpoints and other outputs\n",
    "    eval_strategy=\"steps\",  # Evaluation strategy to use during training ('steps' or 'epochs')\n",
    "    eval_steps=EVAL_STEPS,  # Perform evaluation every 500 steps\n",
    "    num_train_epochs=TRAIN_EPOCHS,  # Total number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training on each device\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation on each device\n",
    "    learning_rate=2.5e-4,  # Initial learning rate for the optimizer\n",
    "    lr_scheduler_type='cosine',  # Learning rate scheduler type. 'cosine' provides a cosine decay schedule.\n",
    "    warmup_ratio=0.05,  # Proportion of training to perform linear learning rate warmup for\n",
    "    adam_beta1=0.9,  # Beta1 parameter for the Adam optimizer (first moment decay)\n",
    "    adam_beta2=0.999,  # Beta2 parameter for the Adam optimizer (second moment decay)\n",
    "    weight_decay=0.01,  # Weight decay to apply (L2 regularization)\n",
    "    logging_strategy=\"steps\",  # Logging strategy to use. 'steps' logs at specified steps.\n",
    "    logging_steps=EVAL_STEPS,  # Log training metrics every 500 steps\n",
    "    save_steps=SAVE_STEPS,  # Save a checkpoint every 1000 steps\n",
    "    save_total_limit=10,  # Maximum number of checkpoints to keep. Older checkpoints are deleted.\n",
    "    # report_to='wandb',  # Uncomment to report metrics to Weights and Biases (optional)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88001ef1-ee67-4a65-82b1-ab1f446a9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorWithPadding  # Zkontrolujte, zda máte správný import pro data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,  # Používá se místo tokenizer\n",
    "    train_dataset=chunked_ds[\"train\"],\n",
    "    eval_dataset=chunked_ds[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d45e0-909f-4962-b64d-ec9c9d4b7fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9028/1337411202.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model,\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "trainer = Trainer(model=model,\n",
    "                 args = training_args,\n",
    "                 tokenizer=tokenizer,\n",
    "                 train_dataset=chunked_ds[\"train\"],\n",
    "                 eval_dataset=chunked_ds[\"test\"],\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 data_collator = data_collator)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a47e62b7-cfdc-4d1f-9e03-737a89108290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1110' max='1110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1110/1110 10:51, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.223900</td>\n",
       "      <td>3.882962</td>\n",
       "      <td>48.567674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.674300</td>\n",
       "      <td>3.717967</td>\n",
       "      <td>41.180109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.438000</td>\n",
       "      <td>3.629918</td>\n",
       "      <td>37.709278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.188500</td>\n",
       "      <td>3.590772</td>\n",
       "      <td>36.261607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.858900</td>\n",
       "      <td>3.512029</td>\n",
       "      <td>33.515793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.560100</td>\n",
       "      <td>3.527914</td>\n",
       "      <td>34.052406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.380000</td>\n",
       "      <td>3.528779</td>\n",
       "      <td>34.081874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1110, training_loss=3.277919844893722, metrics={'train_runtime': 652.1319, 'train_samples_per_second': 26.958, 'train_steps_per_second': 1.702, 'total_flos': 4593513922560000.0, 'train_loss': 3.277919844893722, 'epoch': 15.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "197a409c-f945-4679-afd2-474cfc40b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./gpt2-small-final\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f36f3fa-ee6b-4d9e-9232-1ff4a9c1fea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5999d48558445ad917971d847a5f4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Diplomkaazvposlednimsemestru/my_small_gpt2_csknihy/commit/5837da59e23052e4ec6720a18a617970bbb2a3d4', commit_message='Upload tokenizer', commit_description='', oid='5837da59e23052e4ec6720a18a617970bbb2a3d4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Diplomkaazvposlednimsemestru/my_small_gpt2_csknihy', endpoint='https://huggingface.co', repo_type='model', repo_id='Diplomkaazvposlednimsemestru/my_small_gpt2_csknihy'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_MODEL_NAME = \"my_small_gpt2_csknihy\" # change this\n",
    "HF_TOKEN = \"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\"  # change this \n",
    "\n",
    "model.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71907f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  GPT2LMHeadModel, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08c416d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model =  GPT2LMHeadModel.from_pretrained(\"./gpt2-small-final\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0a858e30-34dc-4ffa-9636-aed60200105b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'On jeho vyšej k tomu se něm podporovat jako by bávali. Na druhé zasní tak, životem o představ'}]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = \"On je\" # Set starting prompt, something specific for your dataset\n",
    "\n",
    "generator(\n",
    "    PROMPT,\n",
    "    max_length=50,       # Maximum length of the generated text\n",
    "    do_sample=True,\n",
    "    temperature=0.6,         # Experiment with this\n",
    "    repetition_penalty=1.4,  # Experiment with this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6ff8c",
   "metadata": {},
   "source": [
    "TOHLE MI NEFUNGUHE V LEVO SE VYTVORIL ADRESAR gpt2-traning, ALE NEVYTVORILI SE POZADOVANE ADRESARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "acfff863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_after_N_steps(N, prompt, **kwargs):\n",
    "    model =  GPT2LMHeadModel.from_pretrained(f\"./gpt2-training/checkpoint-{N}/\")\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    output = generator(prompt, **kwargs)\n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4a875690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\n\\n# Cesta k adresáři, který chcete zkontrolovat\\ndirectory = \"./gpt2-training/checkpoint-1000/\"\\n\\n# Zkontroluje existenci adresáře\\nif os.path.isdir(directory):\\n    print(\"Adresář existuje.\")\\nelse:\\n    print(\"Adresář neexistuje.\")\\n'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "\n",
    "# Cesta k adresáři, který chcete zkontrolovat\n",
    "directory = \"./gpt2-training/checkpoint-1000/\"\n",
    "\n",
    "# Zkontroluje existenci adresáře\n",
    "if os.path.isdir(directory):\n",
    "    print(\"Adresář existuje.\")\n",
    "else:\n",
    "    print(\"Adresář neexistuje.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "413cdbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Pokusští počítačře které které'}]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample_after_N_steps(300, \"Pokus\", do_sample=True, temperature=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a364e486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Pokusít a nás nám nedím přík'}]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample_after_N_steps(600, \"Pokus\", do_sample=True, temperature=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "41989141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Pokusu přesně jenom měl jed'}]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample_after_N_steps(900, \"Pokus\", do_sample=True, temperature=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "45a2c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Pokusním těně vypadá, že'}]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample_after_N_steps(1110, \"Pokus\", do_sample=True, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6a1d05e3-b75a-4062-a93f-f1a35a972822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "token = \"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\"  # Vložte svůj token zde\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "06c273b8-4669-401a-a16c-db031715e89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: /teamspace/studios/this_studio/gpt2-small-final\n",
      "Files in directory: ['config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfedd281a894b6bb929dbb9e9c2cbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70e673dc4ea48b7a7de4e54b10649e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Diplomkaazvposlednimsemestru/your-model-name/commit/a0baa1f6c5ab2c378593bbbf03723d64b40afd95', commit_message='Upload tokenizer', commit_description='', oid='a0baa1f6c5ab2c378593bbbf03723d64b40afd95', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Diplomkaazvposlednimsemestru/your-model-name', endpoint='https://huggingface.co', repo_type='model', repo_id='Diplomkaazvposlednimsemestru/your-model-name'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the model directory\n",
    "model_path = os.path.abspath(\"./gpt2-small-final\")\n",
    "\n",
    "# Print the path to verify\n",
    "print(\"Model path:\", model_path)\n",
    "print(\"Files in directory:\", os.listdir(model_path))\n",
    "\n",
    "# Try loading the model and tokenizer again\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "\n",
    "\n",
    "# Push the model and tokenizer to the Hugging Face Hub\n",
    "model.push_to_hub(\"Diplomkaazvposlednimsemestru/your-model-name\")\n",
    "tokenizer.push_to_hub(\"Diplomkaazvposlednimsemestru/your-model-name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "81872caf-f067-4cfd-8884-23dfdf29f8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599f40bd4f9148e9aad3cd962b014308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ef865ffb2d4d718bc71b7bdc335a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/712M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595e1510eb4546cfb63f649d99d5c7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b45ffdcc7f46ad80a6fd38553bbe68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0892240ead42818c9cacc77ae76ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78e9ef8d59c4fa3a01f38c7172fb426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"Toshifumi/bert-base-multilingual-cased-finetuned-emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5f3e9c2c-59df-487e-b366-255a2b3fda1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Bruce Sterling Z·tah na hacker y Napsal: Bruce Sterling, Přeložil: Václav Bárta, 2:423/59.1 Upravil: Martin Hinner, Toto je výtah překladu knihy Bruce Sterlinga \"The Hacker Crackdown\" z časopisu Natura (12/1995 - 07/1996). Originál překladu najdete na adrese: http://www.penguin.cz/~mhi/crackdown/ Vydáno jako knižní příloha ZX Magazínu. Ke stažení na adrese: http://zxm.speccy.cz Literární\n",
      "Emotion: LABEL_1, Score: 0.6464129090309143\n",
      "\n",
      "Text: freeware Bruce Sterling Z·tah na hacker y 5 Z·tah na hackery Hackers manifesto Another one got caught today, it's all over the papers. \"Teenager Arrested in Computer Crime Scandal\", \"Hacker Arrested after Bank Tampering\"... Damn kids. They're all alike. But did you, in your three-piece psychology and 1950's technobrain, ever\n",
      "Emotion: LABEL_0, Score: 0.5150907039642334\n",
      "\n",
      "Text: take a look behind the eyes of the hacker? Did you ever wonder what made him tick, what forces shaped him, what may have molded him? I am a hacker, enter my world... Mine is a world that begins with school... I'm smarter than most of the other kids, this\n",
      "Emotion: LABEL_1, Score: 0.6326249241828918\n",
      "\n",
      "Text: crap they teach us bores me... Damn underachiever. They're all alike. I'm in junior high or high school. I've listened to teachers explain for the fifteenth time how to reduce a fraction. I understand it. \"No, Ms. Smith, I didn't show my work. I did it in my head...\" Damn\n",
      "Emotion: LABEL_1, Score: 0.7954954504966736\n",
      "\n",
      "Text: kid. Probably copied it. They're all alike. I made a discovery today. I found a computer. Wait a second, this is cool. It does what I want it to. If it makes a mistake, it's because I screwed it up. Not because it doesn't like me... Or feels threatened by\n",
      "Emotion: LABEL_4, Score: 0.7997854351997375\n",
      "\n",
      "Text: me... Or thinks I'm a smart ass... Or doesn't like teaching and shouldn't be here... Damn kid. All he does is play games. They're all alike. And then it happened... a door opened to a world... rushing through the phone line like heroin through an addict's veins, an electronic pulse\n",
      "Emotion: LABEL_3, Score: 0.5888193845748901\n",
      "\n",
      "Text: is sent out, a refuge from the day-to-day incompe- tencies is sought... a board is found. \"This is it... this is where I belong...\" I know everyone here... even if I've never met them, never talked to them, may never hear from them again... I know you all... Damn kid.\n",
      "Emotion: LABEL_1, Score: 0.4629666805267334\n",
      "\n",
      "Text: Tying up the phone line again. They're all alike... You bet your ass we're all alike... we've been spoon-fed baby food at school when we hunge- red for steak... the bits of meat that you did let slip through were pre-chewed and tasteless. We've been dominated by sadists, or ignored\n",
      "Emotion: LABEL_0, Score: 0.643509030342102\n",
      "\n",
      "Text: by the apathetic. The few that had something to teach found us willing pupils, but those few are like drops of water in the desert. This is our world now... the world of the electron and the switch, the beauty of the baud. We make use of a service already\n",
      "Emotion: LABEL_1, Score: 0.9355984926223755\n",
      "\n",
      "Text: existing without paying for what could be dirt-cheap if it wasn't run by profiteering gluttons, and you call us criminals. We explore... and you call us criminals. We seek after knowledge... and you call us criminals. We exist without skin color, without nationality, without religious bias... and you call us\n",
      "Emotion: LABEL_1, Score: 0.4906678795814514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "# Načteme model pro analýzu emocí\n",
    "pipe = pipeline(\"text-classification\", model=\"Toshifumi/bert-base-multilingual-cased-finetuned-emotion\")\n",
    "\n",
    "# Načteme dataset z Hugging Face Hub\n",
    "dataset = load_dataset(\"Diplomkaazvposlednimsemestru/MUNIAI\")  # Změňte 'your_dataset_name' na název datasetu\n",
    "\n",
    "# Získáme texty z datasetu (předpokládejme, že jsou ve sloupci 'text')\n",
    "texts = dataset['train']['text']  # Pokud máte trénovací data, nebo použijte 'test' nebo 'validation'\n",
    "\n",
    "# Aplikujeme model na texty\n",
    "results = pipe(texts[:10])  # Například analyzujeme prvních 10 textů\n",
    "\n",
    "# Výpis výsledků\n",
    "for text, result in zip(texts[:10], results):  # Vybereme jen prvních 10 textů pro zobrazení\n",
    "    print(f\"Text: {text}\\nEmotion: {result['label']}, Score: {result['score']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
